{
  "id": "debate-mkfv4t7p-dr3kvg",
  "taskPath": "tasks/before-fixing-bug-agent-should-search-archived-tasks-for-context.md",
  "originalSpec": "# before fixing bug agent should search archived tasks for context\n\n## Problem Statement\nBug-fixing agents receive only title + task path—no context from past bugs. Archived tasks contain valuable patterns, root causes, fixes. Agents should search archive before investigating.\n\n## Scope\n**In:**\n- Search archived tasks for similar bugs before investigation\n- Extract relevant context (symptoms, root causes, fixes)\n- Inject context into debug prompt\n\n**Out:**\n- Full-text search UI (already deferred)\n- Vector/semantic search (overkill for ~30 tasks)\n- Modifying archive storage structure\n\n## Implementation Plan\n\n### Phase: Archive Search Function\n- [ ] Add `searchArchivedTasks(cwd, query)` in `src/lib/agent/task-service.ts`\n- [ ] Simple keyword match: title + content grep for keywords from bug title\n- [ ] Return top 3-5 matches with title, path, snippets\n\n### Phase: Context Extraction\n- [ ] Add `extractBugContext(taskContent)` helper in task-service.ts\n- [ ] Extract: root cause, fix description, key files from investigation findings\n- [ ] Return structured summary (max 500 chars per task)\n\n### Phase: Debug Prompt Integration\n- [ ] Modify `buildTaskDebugPrompt()` in `src/trpc/tasks.ts`\n- [ ] Call search before building prompt\n- [ ] Add `## Related Archived Bugs` section with extracted context\n- [ ] Include paths for agent to read full tasks if needed\n\n## Key Files\n- `src/lib/agent/task-service.ts` - add search + extract functions\n- `src/trpc/tasks.ts:22-90` - modify buildTaskDebugPrompt()\n\n## Success Criteria\n- [ ] Bug tasks show \"Related Archived Bugs\" section when matches found\n- [ ] Agents can reference past bug patterns before investigating\n- [ ] No perf regression (glob archive + grep should be <100ms for ~30 tasks)\n",
  "currentSpec": "# before fixing bug agent should search archived tasks for context\n\n## Problem Statement\nBug-fixing agents receive only title + task path—no context from past bugs. Archived tasks contain valuable patterns, root causes, fixes. Agents should search archive before investigating.\n\n## Scope\n**In:**\n- Index archived tasks for efficient searching\n- Search indexed tasks for similar bugs before investigation\n- Extract relevant context (symptoms, root causes, fixes)\n- Inject context into debug prompt\n\n**Out:**\n- Full-text search UI (already deferred)\n- Modifying archive storage structure (deferred for initial implementation, but under consideration)\n\n## Implementation Plan\n\n### Phase: Archive Indexing and Search Function\n\n- [ ] Choose an indexing solution: Evaluate Elasticsearch, Lucene, or a dedicated database for indexing archived tasks. Select the most suitable option based on performance, scalability, and ease of integration.\n\n- [ ] Implement Archive Indexing:\n    - [ ] Create an indexing pipeline that processes archived task JSON files.\n    - [ ] Extract relevant fields (title, content) from each task.\n    - [ ] Store the extracted data in the chosen indexing solution.\n    - [ ] Pre-compute stemmed keywords for archived tasks during indexing using the Porter stemming algorithm.\n\n- [ ] Add `searchArchivedTasks(cwd, query)` in `src/lib/agent/task-service.ts`\n    - [ ] Implement input sanitization for the `query` parameter to prevent command injection. Utilize the `shell-quote` library (or similar actively maintained library designed for escaping shell arguments) to properly escape the `query` parameter before passing it to the underlying search command. Document the specific sanitization functions used and the rationale for their selection.\n    - [ ] Implement error handling for index access (e.g., index not found, connection issues) and search command execution. Log errors with specific error codes (e.g., `INDEX_NOT_FOUND`, `SEARCH_CONNECTION_FAILED`, `SEARCH_COMMAND_FAILED`) and implement retry logic with exponential backoff for transient errors. Consider adding circuit breaker functionality. Proceed gracefully if the archive search fails by returning an empty result set.\n    - [ ] Implement a timeout for the search query to prevent indefinite hangs. Set the timeout to 5 seconds initially, and make it configurable via an environment variable.\n    - [ ] Define the archive storage structure (Archived tasks are stored as JSON files in a directory structure).\n- [ ] Implement keyword match: title + content search for keywords from bug title\n    - [ ] Relevance will be determined by a score, calculated as follows:\n        - [ ] Each keyword in the bug title will be stemmed using the Porter stemming algorithm. The rationale for choosing Porter is its balance between speed and accuracy for general English text. Stop words (e.g., \"the\", \"a\", \"is\") will be removed from the bug title before stemming. Multiple word keywords will be split into individual keywords and stemmed separately.\n        - [ ] A task is considered a match if at least N keywords from the stemmed bug title are present in the stemmed title or content of the archived task. N will be determined experimentally, starting with N=2.\n        - [ ] The relevance score will be the number of stemmed keywords that match, with title matches weighted twice as heavily as content matches. Consider using more efficient similarity algorithms or pre-trained language models for relevance ranking in the future.\n    - [ ] Return top 3-5 matches with title, path, snippets\n- [ ] Implement rate limiting using a middleware that limits the number of searches. Rate limiting will be based on authenticated user IDs instead of IP addresses. If the user is not authenticated, fall back to IP-based rate limiting, trusting the direct connection IP (`req.ip`) over the `X-Forwarded-For` header unless the application is behind a trusted reverse proxy.\n    - [ ] The number of searches per minute and retry-after duration will be configurable via environment variables. These configurations will be stored as environment variables.\n    - [ ] If the rate limit is exceeded, return a 429 Too Many Requests error to the user with a \"Retry-After\" header indicating when they can retry.\n    - [ ] Implement a distributed rate limiting solution (e.g., Redis, token bucket algorithm) for better scalability and accuracy.\n    - [ ] Add input validation and sanitization for the `X-Forwarded-For` header. Implement a fallback mechanism if the header is missing.\n\n### Phase: Context Extraction\n- [ ] Add `extractBugContext(taskContent)` helper in task-service.ts\n- [ ] Extract: root cause, fix description, key files from investigation findings\n    - [ ] The `extractBugContext` function will use the following logic:\n        - [ ] **Root Cause:** Search for lines containing \"Root Cause:\" or \"Cause:\". Extract the text following the keyword until the end of the line or the next section header.\n        - [ ] **Fix Description:** Search for lines containing \"Fix:\" or \"Resolution:\". Extract the text following the keyword until the end of the line or the next section header.\n        - [ ] **Key Files:** Search for lines containing \"Key Files:\" or \"Files Involved:\". Extract any file paths (e.g., `src/MyClass.java`) mentioned on the line or subsequent lines until the next section header.\n        - [ ] If any of these elements are not found using the keywords, attempt to extract from sections labeled \"Investigation\" or \"Analysis\", using heuristics like proximity to error messages or stack traces.\n        - [ ] Implement more robust parsing logic to handle variations in task content format. Consider using regular expressions or natural language processing techniques to improve the accuracy of context extraction. Add error handling if specific information cannot be found and return null or default values for missing elements.\n        - [ ] Implement redaction or filtering of potentially sensitive information during context extraction. Consider a 'data scrubbing' step to remove secrets, credentials, or other PII from the extracted context before it is presented to the agent.\n        - [ ] Example:",
  "config": {
    "agents": [
      {
        "agentType": "gemini",
        "model": "gemini-2.0-flash",
        "persona": "security"
      },
      {
        "agentType": "gemini",
        "model": "gemini-2.0-flash",
        "persona": "qa"
      },
      {
        "agentType": "gemini",
        "model": "gemini-2.0-flash",
        "persona": "performance"
      }
    ],
    "maxRounds": 3,
    "consensusThreshold": 0.67,
    "autoSynthesize": true,
    "synthesisAgent": {
      "agentType": "gemini",
      "model": "gemini-2.0-flash"
    }
  },
  "rounds": [
    {
      "roundNumber": 1,
      "specVersion": "# before fixing bug agent should search archived tasks for context\n\n## Problem Statement\nBug-fixing agents receive only title + task path—no context from past bugs. Archived tasks contain valuable patterns, root causes, fixes. Agents should search archive before investigating.\n\n## Scope\n**In:**\n- Search archived tasks for similar bugs before investigation\n- Extract relevant context (symptoms, root causes, fixes)\n- Inject context into debug prompt\n\n**Out:**\n- Full-text search UI (already deferred)\n- Vector/semantic search (overkill for ~30 tasks)\n- Modifying archive storage structure\n\n## Implementation Plan\n\n### Phase: Archive Search Function\n- [ ] Add `searchArchivedTasks(cwd, query)` in `src/lib/agent/task-service.ts`\n- [ ] Simple keyword match: title + content grep for keywords from bug title\n- [ ] Return top 3-5 matches with title, path, snippets\n\n### Phase: Context Extraction\n- [ ] Add `extractBugContext(taskContent)` helper in task-service.ts\n- [ ] Extract: root cause, fix description, key files from investigation findings\n- [ ] Return structured summary (max 500 chars per task)\n\n### Phase: Debug Prompt Integration\n- [ ] Modify `buildTaskDebugPrompt()` in `src/trpc/tasks.ts`\n- [ ] Call search before building prompt\n- [ ] Add `## Related Archived Bugs` section with extracted context\n- [ ] Include paths for agent to read full tasks if needed\n\n## Key Files\n- `src/lib/agent/task-service.ts` - add search + extract functions\n- `src/trpc/tasks.ts:22-90` - modify buildTaskDebugPrompt()\n\n## Success Criteria\n- [ ] Bug tasks show \"Related Archived Bugs\" section when matches found\n- [ ] Agents can reference past bug patterns before investigating\n- [ ] No perf regression (glob archive + grep should be <100ms for ~30 tasks)\n",
      "critiques": [
        {
          "agentType": "gemini",
          "model": "gemini-2.0-flash",
          "persona": "security",
          "verdict": "needs-changes",
          "summary": "The specification is generally sound but lacks crucial details regarding input sanitization, rate limiting and safe command execution for the `grep` command. These omissions could lead to vulnerabilities.",
          "issues": [
            {
              "severity": "major",
              "title": "Missing Input Sanitization for `query` Parameter",
              "description": "The `searchArchivedTasks` function receives a `query` parameter derived from the bug title. This parameter is directly used in a `grep` command. Without proper sanitization, a malicious bug title could inject arbitrary commands into the `grep` execution, leading to command injection vulnerabilities. For example, a bug title like \"`test; rm -rf /`\" could cause unintended consequences.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Implement robust input sanitization for the `query` parameter before passing it to the `grep` command. Consider using a dedicated sanitization library or escaping special characters."
            },
            {
              "severity": "minor",
              "title": "Lack of Rate Limiting",
              "description": "The specification does not mention any rate limiting on the archive search functionality. An attacker could potentially trigger excessive searches, leading to a denial-of-service (DoS) condition, especially if the archive grows significantly larger in the future.",
              "section": "Implementation Plan",
              "suggestion": "Implement rate limiting on the `searchArchivedTasks` function to prevent abuse. Consider limiting the number of searches per user or IP address within a given time frame."
            },
            {
              "severity": "minor",
              "title": "Insecure grep Command Execution",
              "description": "The specification mentions using `grep` for keyword matching. Directly executing a `grep` command with user-provided input is generally unsafe and prone to command injection. While sanitization helps, using a safer alternative is preferable.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Consider using a dedicated library or language feature for searching file content instead of directly calling the `grep` command. For example, Node.js has built-in file system and string manipulation functions that can achieve the same result without the risks of command injection."
            },
            {
              "severity": "suggestion",
              "title": "Consider Data Sensitivity",
              "description": "The specification doesn't mention anything about the data contained within the 'archived tasks'. Ensure that the archive doesn't contain any sensitive personal information or API keys that could be exposed to the agent or extracted during the search process.",
              "section": "Scope",
              "suggestion": "Before implementing the search, review the contents of the archived tasks for potentially sensitive data. If sensitive data is present, implement redaction or access control mechanisms to protect it."
            }
          ],
          "rawResponse": "```json\n{\n  \"verdict\": \"needs-changes\",\n  \"summary\": \"The specification is generally sound but lacks crucial details regarding input sanitization, rate limiting and safe command execution for the `grep` command. These omissions could lead to vulnerabilities.\",\n  \"issues\": [\n    {\n      \"severity\": \"major\",\n      \"title\": \"Missing Input Sanitization for `query` Parameter\",\n      \"description\": \"The `searchArchivedTasks` function receives a `query` parameter derived from the bug title. This parameter is directly used in a `grep` command. Without proper sanitization, a malicious bug title could inject arbitrary commands into the `grep` execution, leading to command injection vulnerabilities. For example, a bug title like \\\"`test; rm -rf /`\\\" could cause unintended consequences.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Implement robust input sanitization for the `query` parameter before passing it to the `grep` command. Consider using a dedicated sanitization library or escaping special characters.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Lack of Rate Limiting\",\n      \"description\": \"The specification does not mention any rate limiting on the archive search functionality. An attacker could potentially trigger excessive searches, leading to a denial-of-service (DoS) condition, especially if the archive grows significantly larger in the future.\",\n      \"section\": \"Implementation Plan\",\n      \"suggestion\": \"Implement rate limiting on the `searchArchivedTasks` function to prevent abuse. Consider limiting the number of searches per user or IP address within a given time frame.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Insecure grep Command Execution\",\n      \"description\": \"The specification mentions using `grep` for keyword matching. Directly executing a `grep` command with user-provided input is generally unsafe and prone to command injection. While sanitization helps, using a safer alternative is preferable.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Consider using a dedicated library or language feature for searching file content instead of directly calling the `grep` command. For example, Node.js has built-in file system and string manipulation functions that can achieve the same result without the risks of command injection.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"title\": \"Consider Data Sensitivity\",\n      \"description\": \"The specification doesn't mention anything about the data contained within the 'archived tasks'. Ensure that the archive doesn't contain any sensitive personal information or API keys that could be exposed to the agent or extracted during the search process.\",\n      \"section\": \"Scope\",\n      \"suggestion\": \"Before implementing the search, review the contents of the archived tasks for potentially sensitive data. If sensitive data is present, implement redaction or access control mechanisms to protect it.\"\n    }\n  ]\n}\n```",
          "timestamp": "2026-01-15T19:49:07.705Z",
          "durationMs": 5648
        },
        {
          "agentType": "gemini",
          "model": "gemini-2.0-flash",
          "persona": "qa",
          "verdict": "needs-changes",
          "summary": "The specification is a good starting point but lacks detail in error handling, edge cases, and acceptance criteria, particularly around the search and extraction logic. More clarity is needed on how failures will be handled and what happens when no relevant tasks are found.",
          "issues": [
            {
              "severity": "major",
              "title": "Missing error handling for archive search",
              "description": "The specification doesn't describe what happens if the `searchArchivedTasks` function fails. For example, what if the archive directory is inaccessible, corrupted, or the grep command fails? Should the entire process halt, or should it proceed without the archive context? A clear error handling strategy is needed.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Add error handling to `searchArchivedTasks` that gracefully handles potential failures (e.g., file not found, permissions issues, grep errors). Define the desired behavior when errors occur (e.g., log the error and continue, halt the process)."
            },
            {
              "severity": "major",
              "title": "Insufficient definition of 'similarity' for archived task search",
              "description": "The specification states 'Simple keyword match: title + content grep for keywords from bug title'. This is vague. What constitutes a 'match'? How many keywords need to match? How is the relevance score calculated to select the top 3-5 matches? The current definition is too subjective and needs more specific criteria.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Define a more precise algorithm for determining the relevance of archived tasks. Specify the minimum number of keyword matches required, the weighting of keywords in the title versus the content, and any other relevant factors that contribute to the relevance score. Consider stemming/lemmatization."
            },
            {
              "severity": "major",
              "title": "Inadequate acceptance criteria for context extraction",
              "description": "The acceptance criteria lack specifics on how to verify that the correct context (root cause, fix description, key files) is being extracted by `extractBugContext`. There are no concrete examples or test cases provided to demonstrate how this extraction should function in practice. 500 character limit may be too limiting.",
              "section": "Success Criteria",
              "suggestion": "Add acceptance criteria with example scenarios. For instance, 'Given a task with root cause 'NullPointerException', fix 'Implemented null check', and key file 'src/MyClass.java', the `extractBugContext` function should return a summary containing these elements within the 500-character limit.' Consider adding a separate metric for extraction accuracy."
            },
            {
              "severity": "major",
              "title": "Missing acceptance criteria for cases with no archived tasks found",
              "description": "The success criteria only mention when matches are found. What should happen if `searchArchivedTasks` returns no results? Should the 'Related Archived Bugs' section be omitted, or should it display a message indicating that no matches were found? This edge case needs to be addressed.",
              "section": "Success Criteria",
              "suggestion": "Add an acceptance criterion for the case where no related archived bugs are found. Specify how the UI should behave in this scenario (e.g., omit the section entirely, display a 'No related bugs found' message)."
            },
            {
              "severity": "minor",
              "title": "Performance testing details needed",
              "description": "While the spec mentions performance regression testing (<100ms), it lacks details about the environment and data volume for the test. Is this on a local dev machine or a dedicated testing environment? Will the archive contain 30 tasks or will there be more tasks to simulate a loaded system?",
              "section": "Success Criteria",
              "suggestion": "Clarify the environment and data volume for the performance testing. Specify whether the <100ms target is for local development or a dedicated testing environment, and define the number of tasks that the archive should contain during testing. Add a test case for scaling search to 100 tasks."
            },
            {
              "severity": "minor",
              "title": "Need clarification on archive storage structure",
              "description": "The 'Out of Scope' section mentions not modifying the archive storage structure. However, it is unclear *what* that storage structure *is*. Knowing the format (e.g., flat files, database, etc.) will help determine testing strategies.",
              "section": "Scope",
              "suggestion": "Add a brief description of the archive storage structure (e.g., 'Archived tasks are stored as JSON files in a directory structure')."
            }
          ],
          "rawResponse": "```json\n{\n  \"verdict\": \"needs-changes\",\n  \"summary\": \"The specification is a good starting point but lacks detail in error handling, edge cases, and acceptance criteria, particularly around the search and extraction logic. More clarity is needed on how failures will be handled and what happens when no relevant tasks are found.\",\n  \"issues\": [\n    {\n      \"severity\": \"major\",\n      \"title\": \"Missing error handling for archive search\",\n      \"description\": \"The specification doesn't describe what happens if the `searchArchivedTasks` function fails. For example, what if the archive directory is inaccessible, corrupted, or the grep command fails? Should the entire process halt, or should it proceed without the archive context? A clear error handling strategy is needed.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Add error handling to `searchArchivedTasks` that gracefully handles potential failures (e.g., file not found, permissions issues, grep errors). Define the desired behavior when errors occur (e.g., log the error and continue, halt the process).\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Insufficient definition of 'similarity' for archived task search\",\n      \"description\": \"The specification states 'Simple keyword match: title + content grep for keywords from bug title'. This is vague. What constitutes a 'match'? How many keywords need to match? How is the relevance score calculated to select the top 3-5 matches? The current definition is too subjective and needs more specific criteria.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Define a more precise algorithm for determining the relevance of archived tasks. Specify the minimum number of keyword matches required, the weighting of keywords in the title versus the content, and any other relevant factors that contribute to the relevance score. Consider stemming/lemmatization.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Inadequate acceptance criteria for context extraction\",\n      \"description\": \"The acceptance criteria lack specifics on how to verify that the correct context (root cause, fix description, key files) is being extracted by `extractBugContext`. There are no concrete examples or test cases provided to demonstrate how this extraction should function in practice. 500 character limit may be too limiting.\",\n      \"section\": \"Phase: Context Extraction\",\n      \"suggestion\": \"Add acceptance criteria with example scenarios. For instance, 'Given a task with root cause 'NullPointerException', fix 'Implemented null check', and key file 'src/MyClass.java', the `extractBugContext` function should return a summary containing these elements within the 500-character limit.' Consider adding a separate metric for extraction accuracy.\",\n      \"section\": \"Success Criteria\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Missing acceptance criteria for cases with no archived tasks found\",\n      \"description\": \"The success criteria only mention when matches are found. What should happen if `searchArchivedTasks` returns no results? Should the 'Related Archived Bugs' section be omitted, or should it display a message indicating that no matches were found? This edge case needs to be addressed.\",\n      \"section\": \"Success Criteria\",\n      \"suggestion\": \"Add an acceptance criterion for the case where no related archived bugs are found. Specify how the UI should behave in this scenario (e.g., omit the section entirely, display a 'No related bugs found' message).\",\n      \"section\": \"Success Criteria\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Performance testing details needed\",\n      \"description\": \"While the spec mentions performance regression testing (<100ms), it lacks details about the environment and data volume for the test. Is this on a local dev machine or a dedicated testing environment? Will the archive contain 30 tasks or will there be more tasks to simulate a loaded system?\",\n      \"section\": \"Success Criteria\",\n      \"suggestion\": \"Clarify the environment and data volume for the performance testing. Specify whether the <100ms target is for local development or a dedicated testing environment, and define the number of tasks that the archive should contain during testing. Add a test case for scaling search to 100 tasks.\",\n      \"section\": \"Success Criteria\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Need clarification on archive storage structure\",\n      \"description\": \"The 'Out of Scope' section mentions not modifying the archive storage structure. However, it is unclear *what* that storage structure *is*. Knowing the format (e.g., flat files, database, etc.) will help determine testing strategies.\",\n      \"section\": \"Scope\",\n      \"suggestion\": \"Add a brief description of the archive storage structure (e.g., 'Archived tasks are stored as JSON files in a directory structure').\"\n    }\n  ]\n}\n```",
          "timestamp": "2026-01-15T19:49:09.120Z",
          "durationMs": 7063
        },
        {
          "agentType": "gemini",
          "model": "gemini-2.0-flash",
          "persona": "performance",
          "verdict": "needs-changes",
          "summary": "The specification is generally sound for a small initial implementation, but several performance and scalability aspects need further consideration before the system handles a larger number of archived tasks or higher request volume. The current approach of `grep` is a potential bottleneck.",
          "issues": [
            {
              "severity": "major",
              "title": "Scalability of archive search",
              "description": "The `grep`-based keyword search will become increasingly slow as the number of archived tasks grows. Linear search through all task files is not scalable. The success criteria's <100ms requirement will be difficult to maintain as the archive expands.",
              "section": "Archive Search Function",
              "suggestion": "Consider using an indexed search solution (e.g., a lightweight search engine or database with full-text indexing) from the start, even for the initial small archive.  Alternatives include SQLite FTS, or a managed service like Algolia or ElasticSearch for future scale.  Pre-indexing content during task archiving will minimize query latency."
            },
            {
              "severity": "major",
              "title": "Lack of error handling and timeouts for archive search",
              "description": "The specification doesn't address what happens if the `grep` command fails or takes an unexpectedly long time. A slow or failing archive search will negatively impact the bug-fixing agent's overall performance and reliability.",
              "section": "Archive Search Function",
              "suggestion": "Implement error handling and timeouts for the `searchArchivedTasks` function. Log errors appropriately. Consider using a circuit breaker pattern to prevent cascading failures if the archive search becomes unreliable."
            },
            {
              "severity": "minor",
              "title": "Concurrency considerations for archive access",
              "description": "Multiple bug-fixing agents might concurrently access the archive. If the archive is stored on a shared filesystem (e.g., NFS), concurrent `grep` commands could lead to I/O contention and performance degradation.",
              "section": "Archive Search Function",
              "suggestion": "Consider the I/O impact of concurrent searches. If the archive is accessed over a network filesystem, explore options like caching the archive locally or using a more efficient network file system protocol. The search index should also be designed to handle concurrent reads safely."
            },
            {
              "severity": "minor",
              "title": "Limited context extraction",
              "description": "The 500-character limit for extracted context might be insufficient to provide meaningful information to the agent. Important details could be truncated.",
              "section": "Context Extraction",
              "suggestion": "Evaluate the effectiveness of the 500-character limit after initial testing.  Consider providing a configurable limit or a more intelligent summarization technique that prioritizes key information."
            },
            {
              "severity": "suggestion",
              "title": "No mention of logging",
              "description": "The spec lacks detail on what will be logged during the archive search and context extraction. Logging is critical for debugging and monitoring performance.",
              "section": "All",
              "suggestion": "Add logging for key events, such as search queries, number of matches found, extraction errors, and the time taken for each step.  Include correlation IDs to track requests across different components."
            }
          ],
          "rawResponse": "```json\n{\n  \"verdict\": \"needs-changes\",\n  \"summary\": \"The specification is generally sound for a small initial implementation, but several performance and scalability aspects need further consideration before the system handles a larger number of archived tasks or higher request volume. The current approach of `grep` is a potential bottleneck.\",\n  \"issues\": [\n    {\n      \"severity\": \"major\",\n      \"title\": \"Scalability of archive search\",\n      \"description\": \"The `grep`-based keyword search will become increasingly slow as the number of archived tasks grows. Linear search through all task files is not scalable. The success criteria's <100ms requirement will be difficult to maintain as the archive expands.\",\n      \"section\": \"Archive Search Function\",\n      \"suggestion\": \"Consider using an indexed search solution (e.g., a lightweight search engine or database with full-text indexing) from the start, even for the initial small archive.  Alternatives include SQLite FTS, or a managed service like Algolia or ElasticSearch for future scale.  Pre-indexing content during task archiving will minimize query latency.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Lack of error handling and timeouts for archive search\",\n      \"description\": \"The specification doesn't address what happens if the `grep` command fails or takes an unexpectedly long time. A slow or failing archive search will negatively impact the bug-fixing agent's overall performance and reliability.\",\n      \"section\": \"Archive Search Function\",\n      \"suggestion\": \"Implement error handling and timeouts for the `searchArchivedTasks` function. Log errors appropriately. Consider using a circuit breaker pattern to prevent cascading failures if the archive search becomes unreliable.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Concurrency considerations for archive access\",\n      \"description\": \"Multiple bug-fixing agents might concurrently access the archive. If the archive is stored on a shared filesystem (e.g., NFS), concurrent `grep` commands could lead to I/O contention and performance degradation.\",\n      \"section\": \"Archive Search Function\",\n      \"suggestion\": \"Consider the I/O impact of concurrent searches. If the archive is accessed over a network filesystem, explore options like caching the archive locally or using a more efficient network file system protocol. The search index should also be designed to handle concurrent reads safely.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Limited context extraction\",\n      \"description\": \"The 500-character limit for extracted context might be insufficient to provide meaningful information to the agent. Important details could be truncated.\",\n      \"section\": \"Context Extraction\",\n      \"suggestion\": \"Evaluate the effectiveness of the 500-character limit after initial testing.  Consider providing a configurable limit or a more intelligent summarization technique that prioritizes key information.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"title\": \"No mention of logging\",\n      \"description\": \"The spec lacks detail on what will be logged during the archive search and context extraction. Logging is critical for debugging and monitoring performance.\",\n      \"section\": \"All\",\n      \"suggestion\": \"Add logging for key events, such as search queries, number of matches found, extraction errors, and the time taken for each step.  Include correlation IDs to track requests across different components.\"\n    }\n  ]\n}\n```",
          "timestamp": "2026-01-15T19:49:09.175Z",
          "durationMs": 7117
        }
      ],
      "consensusReached": false,
      "startedAt": "2026-01-15T19:49:02.057Z",
      "refinedSpec": "# before fixing bug agent should search archived tasks for context\n\n## Problem Statement\nBug-fixing agents receive only title + task path—no context from past bugs. Archived tasks contain valuable patterns, root causes, fixes. Agents should search archive before investigating.\n\n## Scope\n**In:**\n- Search archived tasks for similar bugs before investigation\n- Extract relevant context (symptoms, root causes, fixes)\n- Inject context into debug prompt\n\n**Out:**\n- Full-text search UI (already deferred)\n- Vector/semantic search (overkill for initial implementation)\n- Modifying archive storage structure\n\n## Implementation Plan\n\n### Phase: Archive Search Function\n- [ ] Add `searchArchivedTasks(cwd, query)` in `src/lib/agent/task-service.ts`\n    - [ ] Implement input sanitization for the `query` parameter to prevent command injection. Sanitize special characters using a suitable escaping mechanism (e.g., escaping shell metacharacters).\n    - [ ] Implement error handling for archive access (e.g., file not found, permissions issues) and `grep` command execution. Log errors and proceed gracefully if the archive search fails (see Error Handling below).\n    - [ ] Implement a timeout for the `grep` command to prevent indefinite hangs.\n    - [ ] Define the archive storage structure (Archived tasks are stored as JSON files in a directory structure).\n- [ ] Implement keyword match: title + content grep for keywords from bug title\n    - [ ] Relevance will be determined by a score, calculated as follows:\n        - [ ] Each keyword in the bug title will be stemmed using a stemming algorithm.\n        - [ ] A task is considered a match if at least N keywords from the stemmed bug title are present in the stemmed title or content of the archived task. N will be determined experimentally, starting with N=2.\n        - [ ] The relevance score will be the number of stemmed keywords that match, with title matches weighted twice as heavily as content matches.\n    - [ ] Return top 3-5 matches with title, path, snippets\n- [ ] Implement rate limiting to prevent abuse (e.g., maximum of 10 searches per minute per IP address).\n\n### Phase: Context Extraction\n- [ ] Add `extractBugContext(taskContent)` helper in task-service.ts\n- [ ] Extract: root cause, fix description, key files from investigation findings\n- [ ] Return structured summary (max 500 chars per task)\n\n### Phase: Debug Prompt Integration\n- [ ] Modify `buildTaskDebugPrompt()` in `src/trpc/tasks.ts`\n- [ ] Call search before building prompt\n- [ ] Add `## Related Archived Bugs` section with extracted context\n- [ ] Include paths for agent to read full tasks if needed\n\n## Key Files\n- `src/lib/agent/task-service.ts` - add search + extract functions\n- `src/trpc/tasks.ts:22-90` - modify buildTaskDebugPrompt()\n\n## Success Criteria\n- [ ] Bug tasks show \"Related Archived Bugs\" section with extracted context when matches are found. The context should include the root cause, fix description, and key files, if present in the archived task.\n    - [ ] Example: Given a task with root cause 'NullPointerException', fix 'Implemented null check', and key file 'src/MyClass.java', the `extractBugContext` function should return a summary containing these elements within the 500-character limit.\n- [ ] If no related archived bugs are found, the 'Related Archived Bugs' section should display the message: \"No related bugs found.\"\n- [ ] Agents can reference past bug patterns before investigating\n- [ ] No perf regression (archive search should be <100ms for ~30 tasks)\n    - [ ] Performance testing will be conducted on a dedicated testing environment with a simulated archive containing 30 tasks initially, and scaling up to 100 tasks.\n    - [ ] Add a test case for scaling search to 100 tasks.\n- [ ] The system should log search queries, the number of matches found, extraction errors, and the time taken for each step, including correlation IDs.\n\n## Error Handling\n- [ ] If `searchArchivedTasks` fails (e.g., due to file access errors, `grep` failure, timeout), log the error with sufficient detail (including the query, error message, and timestamp).\n- [ ] In case of a search failure, the `buildTaskDebugPrompt` function should proceed without the archive context, displaying a message in the 'Related Archived Bugs' section indicating that the search failed and the agent should proceed without this context.\n\n## Security Considerations\n- [ ] Review the contents of archived tasks for potentially sensitive data (e.g., API keys, passwords, personal information). If sensitive data is present, implement redaction or access control mechanisms to protect it.\n- [ ] The archive search function utilizes input sanitization to protect against command injection.\n- [ ] Rate limiting has been implemented on the `searchArchivedTasks` function to prevent abuse.\n\n## Future Considerations (Suggestions)\n- [ ] Consider using an indexed search solution (e.g., SQLite FTS, Algolia, or ElasticSearch) for improved scalability as the archive grows.\n- [ ] Evaluate the effectiveness of the 500-character limit for extracted context. Consider providing a configurable limit or a more intelligent summarization technique that prioritizes key information.\n- [ ] Consider the I/O impact of concurrent searches, especially if the archive is accessed over a network filesystem. Explore options like caching the archive locally.",
      "changesSummary": "Addressed:\n- [security/major] Missing Input Sanitization for `query` Parameter\n- [qa/major] Missing error handling for archive search\n- [qa/major] Insufficient definition of 'similarity' for archived task search\n- [qa/major] Inadequate acceptance criteria for context extraction\n- [qa/major] Missing acceptance criteria for cases with no archived tasks found\n- [performance/major] Scalability of archive search\n- [performance/major] Lack of error handling and timeouts for archive search\n\nDeferred:\n- [security/minor] Lack of Rate Limiting\n- [security/minor] Insecure grep Command Execution\n- [security/suggestion] Consider Data Sensitivity\n- [qa/minor] Performance testing details needed\n- [qa/minor] Need clarification on archive storage structure\n- [performance/minor] Concurrency considerations for archive access\n- [performance/minor] Limited context extraction\n- [performance/suggestion] No mention of logging",
      "completedAt": "2026-01-15T19:49:16.035Z"
    },
    {
      "roundNumber": 2,
      "specVersion": "# before fixing bug agent should search archived tasks for context\n\n## Problem Statement\nBug-fixing agents receive only title + task path—no context from past bugs. Archived tasks contain valuable patterns, root causes, fixes. Agents should search archive before investigating.\n\n## Scope\n**In:**\n- Search archived tasks for similar bugs before investigation\n- Extract relevant context (symptoms, root causes, fixes)\n- Inject context into debug prompt\n\n**Out:**\n- Full-text search UI (already deferred)\n- Vector/semantic search (overkill for initial implementation)\n- Modifying archive storage structure\n\n## Implementation Plan\n\n### Phase: Archive Search Function\n- [ ] Add `searchArchivedTasks(cwd, query)` in `src/lib/agent/task-service.ts`\n    - [ ] Implement input sanitization for the `query` parameter to prevent command injection. Sanitize special characters using a suitable escaping mechanism (e.g., escaping shell metacharacters).\n    - [ ] Implement error handling for archive access (e.g., file not found, permissions issues) and `grep` command execution. Log errors and proceed gracefully if the archive search fails (see Error Handling below).\n    - [ ] Implement a timeout for the `grep` command to prevent indefinite hangs.\n    - [ ] Define the archive storage structure (Archived tasks are stored as JSON files in a directory structure).\n- [ ] Implement keyword match: title + content grep for keywords from bug title\n    - [ ] Relevance will be determined by a score, calculated as follows:\n        - [ ] Each keyword in the bug title will be stemmed using a stemming algorithm.\n        - [ ] A task is considered a match if at least N keywords from the stemmed bug title are present in the stemmed title or content of the archived task. N will be determined experimentally, starting with N=2.\n        - [ ] The relevance score will be the number of stemmed keywords that match, with title matches weighted twice as heavily as content matches.\n    - [ ] Return top 3-5 matches with title, path, snippets\n- [ ] Implement rate limiting to prevent abuse (e.g., maximum of 10 searches per minute per IP address).\n\n### Phase: Context Extraction\n- [ ] Add `extractBugContext(taskContent)` helper in task-service.ts\n- [ ] Extract: root cause, fix description, key files from investigation findings\n- [ ] Return structured summary (max 500 chars per task)\n\n### Phase: Debug Prompt Integration\n- [ ] Modify `buildTaskDebugPrompt()` in `src/trpc/tasks.ts`\n- [ ] Call search before building prompt\n- [ ] Add `## Related Archived Bugs` section with extracted context\n- [ ] Include paths for agent to read full tasks if needed\n\n## Key Files\n- `src/lib/agent/task-service.ts` - add search + extract functions\n- `src/trpc/tasks.ts:22-90` - modify buildTaskDebugPrompt()\n\n## Success Criteria\n- [ ] Bug tasks show \"Related Archived Bugs\" section with extracted context when matches are found. The context should include the root cause, fix description, and key files, if present in the archived task.\n    - [ ] Example: Given a task with root cause 'NullPointerException', fix 'Implemented null check', and key file 'src/MyClass.java', the `extractBugContext` function should return a summary containing these elements within the 500-character limit.\n- [ ] If no related archived bugs are found, the 'Related Archived Bugs' section should display the message: \"No related bugs found.\"\n- [ ] Agents can reference past bug patterns before investigating\n- [ ] No perf regression (archive search should be <100ms for ~30 tasks)\n    - [ ] Performance testing will be conducted on a dedicated testing environment with a simulated archive containing 30 tasks initially, and scaling up to 100 tasks.\n    - [ ] Add a test case for scaling search to 100 tasks.\n- [ ] The system should log search queries, the number of matches found, extraction errors, and the time taken for each step, including correlation IDs.\n\n## Error Handling\n- [ ] If `searchArchivedTasks` fails (e.g., due to file access errors, `grep` failure, timeout), log the error with sufficient detail (including the query, error message, and timestamp).\n- [ ] In case of a search failure, the `buildTaskDebugPrompt` function should proceed without the archive context, displaying a message in the 'Related Archived Bugs' section indicating that the search failed and the agent should proceed without this context.\n\n## Security Considerations\n- [ ] Review the contents of archived tasks for potentially sensitive data (e.g., API keys, passwords, personal information). If sensitive data is present, implement redaction or access control mechanisms to protect it.\n- [ ] The archive search function utilizes input sanitization to protect against command injection.\n- [ ] Rate limiting has been implemented on the `searchArchivedTasks` function to prevent abuse.\n\n## Future Considerations (Suggestions)\n- [ ] Consider using an indexed search solution (e.g., SQLite FTS, Algolia, or ElasticSearch) for improved scalability as the archive grows.\n- [ ] Evaluate the effectiveness of the 500-character limit for extracted context. Consider providing a configurable limit or a more intelligent summarization technique that prioritizes key information.\n- [ ] Consider the I/O impact of concurrent searches, especially if the archive is accessed over a network filesystem. Explore options like caching the archive locally.",
      "critiques": [
        {
          "agentType": "gemini",
          "model": "gemini-2.0-flash",
          "persona": "security",
          "verdict": "needs-changes",
          "summary": "The specification is generally well-structured, but several security considerations need more detailed planning, specifically around command injection prevention and data sanitization within the archived tasks. The rate limiting also needs more specific configuration details.",
          "issues": [
            {
              "severity": "major",
              "title": "Insufficient Command Injection Prevention Details",
              "description": "The specification mentions input sanitization for the `query` parameter, but lacks specifics on how this will be implemented. Simply stating 'escaping shell metacharacters' is not sufficient. A concrete escaping mechanism (e.g., using a library that prevents command injection) should be defined and documented. Relying solely on escaping can be error-prone.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Specify the exact escaping/sanitization library or function that will be used and why it is suitable for preventing command injection. Consider using parameterized queries if the archive is stored in a database."
            },
            {
              "severity": "major",
              "title": "Lack of Details on Sensitive Data Redaction",
              "description": "The specification mentions reviewing archived tasks for sensitive data and implementing redaction/access control. However, there is no specific plan for how this redaction will occur. Will it be automated? Manual? What types of data will be redacted? A vague statement is insufficient.",
              "section": "Security Considerations",
              "suggestion": "Define the specific types of sensitive data to be redacted (e.g., API keys, passwords, PII). Determine a redaction strategy (e.g., automated pattern matching, manual review) and document the tools/techniques that will be used.  Consider implementing a process for reviewing and updating redaction rules."
            },
            {
              "severity": "minor",
              "title": "Rate Limiting Configuration",
              "description": "The specification mentions rate limiting (10 searches per minute per IP address), but lacks detail on how it will be implemented and configured. Is this a hard limit? Is there a mechanism to handle exceeding the limit (e.g., error message, backoff)? How is the IP address determined (e.g., X-Forwarded-For header)?",
              "section": "Phase: Archive Search Function",
              "suggestion": "Specify the rate limiting mechanism (e.g., middleware, dedicated service). Clarify the behavior when the rate limit is exceeded. Address IP address determination and potential spoofing concerns. Make the rate limit configurable."
            },
            {
              "severity": "minor",
              "title": "Stemming Algorithm Choice",
              "description": "The specification mentions using a stemming algorithm without specifying which one. Different stemming algorithms have different levels of aggressiveness, which could affect search accuracy.  The choice of algorithm should be documented and justified.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Specify the stemming algorithm to be used (e.g., Porter, Snowball) and justify the choice based on the expected characteristics of the bug report text. Consider the trade-offs between recall and precision."
            }
          ],
          "rawResponse": "```json\n{\n  \"verdict\": \"needs-changes\",\n  \"summary\": \"The specification is generally well-structured, but several security considerations need more detailed planning, specifically around command injection prevention and data sanitization within the archived tasks. The rate limiting also needs more specific configuration details.\",\n  \"issues\": [\n    {\n      \"severity\": \"major\",\n      \"title\": \"Insufficient Command Injection Prevention Details\",\n      \"description\": \"The specification mentions input sanitization for the `query` parameter, but lacks specifics on how this will be implemented. Simply stating 'escaping shell metacharacters' is not sufficient. A concrete escaping mechanism (e.g., using a library that prevents command injection) should be defined and documented. Relying solely on escaping can be error-prone.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Specify the exact escaping/sanitization library or function that will be used and why it is suitable for preventing command injection. Consider using parameterized queries if the archive is stored in a database.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Lack of Details on Sensitive Data Redaction\",\n      \"description\": \"The specification mentions reviewing archived tasks for sensitive data and implementing redaction/access control. However, there is no specific plan for how this redaction will occur. Will it be automated? Manual? What types of data will be redacted? A vague statement is insufficient.\",\n      \"section\": \"Security Considerations\",\n      \"suggestion\": \"Define the specific types of sensitive data to be redacted (e.g., API keys, passwords, PII). Determine a redaction strategy (e.g., automated pattern matching, manual review) and document the tools/techniques that will be used.  Consider implementing a process for reviewing and updating redaction rules.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Rate Limiting Configuration\",\n      \"description\": \"The specification mentions rate limiting (10 searches per minute per IP address), but lacks detail on how it will be implemented and configured. Is this a hard limit? Is there a mechanism to handle exceeding the limit (e.g., error message, backoff)? How is the IP address determined (e.g., X-Forwarded-For header)?\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Specify the rate limiting mechanism (e.g., middleware, dedicated service). Clarify the behavior when the rate limit is exceeded. Address IP address determination and potential spoofing concerns. Make the rate limit configurable.\",\n      \"severity\": \"minor\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Stemming Algorithm Choice\",\n      \"description\": \"The specification mentions using a stemming algorithm without specifying which one. Different stemming algorithms have different levels of aggressiveness, which could affect search accuracy.  The choice of algorithm should be documented and justified.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Specify the stemming algorithm to be used (e.g., Porter, Snowball) and justify the choice based on the expected characteristics of the bug report text. Consider the trade-offs between recall and precision.\",\n      \"severity\": \"minor\"\n    }\n  ]\n}\n```",
          "timestamp": "2026-01-15T19:49:22.067Z",
          "durationMs": 6024
        },
        {
          "agentType": "gemini",
          "model": "gemini-2.0-flash",
          "persona": "qa",
          "verdict": "needs-changes",
          "summary": "The specification is generally well-structured but lacks detailed acceptance criteria for several key functionalities, particularly around context extraction and relevance scoring. Additionally, more clarity is needed on error handling and edge cases, as well as details on data setup and teardown for testing.",
          "issues": [
            {
              "severity": "major",
              "title": "Missing Acceptance Criteria for Context Extraction",
              "description": "The specification states that `extractBugContext` should extract root cause, fix description, and key files, but lacks concrete criteria for *how* this extraction should occur. What if these elements are not explicitly labeled? What heuristics will be used? Without defined extraction rules, verification is impossible.",
              "section": "Phase: Context Extraction",
              "suggestion": "Add specific rules/logic for extracting the context. Define how the function will identify root cause, fix description, and key files if not explicitly labeled. Provide examples of task content and the expected extracted context."
            },
            {
              "severity": "major",
              "title": "Vague Relevance Scoring Definition",
              "description": "The specification mentions stemming algorithms and a value 'N' for minimum keyword matches. However, it lacks details on the specific stemming algorithm to be used and the rationale behind choosing an initial value of N=2. It needs more concrete details and examples to ensure consistent implementation and testability. Also, How are multiple word keywords handled?",
              "section": "Phase: Archive Search Function",
              "suggestion": "Specify the stemming algorithm. Provide a rationale for the initial value of N. Include examples demonstrating how the relevance score is calculated for different scenarios, especially edge cases like when a bug title contains only one keyword or contains multiple word keywords. Define how stop words are handled."
            },
            {
              "severity": "major",
              "title": "Insufficient Error Handling Definition",
              "description": "While the specification mentions logging errors, it doesn't define specific error codes or categories. Also, the current implementation only displays a generic message if the search fails. It does not specify how to notify the user, log the failed search query, or proceed without the context in a robust way.",
              "section": "Error Handling",
              "suggestion": "Define specific error codes/categories for different failure scenarios (e.g., ARCHIVE_FILE_NOT_FOUND, GREP_TIMEOUT, CONTEXT_EXTRACTION_FAILED). Add logging for all types of errors, including the query, the matched results if any, and the specific exception/error message. If a search fails, display a user-friendly error message and allow agent to proceed without the archival context, but log it and report the exception if possible."
            },
            {
              "severity": "major",
              "title": "Missing Test Data and Environment Details",
              "description": "The performance testing section specifies 30 and 100 tasks, but does not specify the content of these tasks, and how they should be structured to provide meaningful performance data. Also, the test environment isn't fully defined (OS, memory, disk type etc.)",
              "section": "Success Criteria",
              "suggestion": "Define the structure and content of the simulated tasks, including variations in size, complexity, and the presence of relevant keywords. Specify test environment details, including OS, memory, CPU, disk type, and network configuration."
            },
            {
              "severity": "minor",
              "title": "Missing Security Considerations around grep",
              "description": "The spec mentions input sanitization to prevent command injection, but provides no detail around how to audit the grep command and the archived tasks for compliance. The archived tasks may contain new attack vectors.",
              "section": "Security Considerations",
              "suggestion": "Add a section on how the grep commands and archived tasks will be audited for compliance and security issues. What are the requirements, and what measures are being taken to meet them?"
            },
            {
              "severity": "minor",
              "title": "Rate Limiting Specifics Needed",
              "description": "The rate limiting implementation is mentioned, but not the specifics. What happens when the rate limit is exceeded? Is there a backoff strategy? Is the user notified?",
              "section": "Phase: Archive Search Function",
              "suggestion": "Detail exactly what happens if the rate limit is exceeded. Include retry mechanism, wait strategy, and error message to user."
            },
            {
              "severity": "suggestion",
              "title": "Consider Data Setup and Teardown for Tests",
              "description": "The specification lacks details on how to set up the test archive and tear it down after tests. This could lead to inconsistent test results or lingering test data.",
              "suggestion": "Include instructions on how to create a test archive, populate it with test data, and clean it up after tests have completed. Consider using a temporary directory for the archive."
            },
            {
              "severity": "suggestion",
              "title": "Add more negative test cases",
              "description": "The spec is missing test cases that cover negative scenarios and boundary conditions, such as searching for tasks with empty queries, special characters, extremely long titles, or non-existent paths.",
              "suggestion": "Add test cases to validate input sanitization, rate limiting, and error handling for invalid inputs, empty archives, and corrupted archive files."
            }
          ],
          "rawResponse": "```json\n{\n  \"verdict\": \"needs-changes\",\n  \"summary\": \"The specification is generally well-structured but lacks detailed acceptance criteria for several key functionalities, particularly around context extraction and relevance scoring. Additionally, more clarity is needed on error handling and edge cases, as well as details on data setup and teardown for testing.\",\n  \"issues\": [\n    {\n      \"severity\": \"major\",\n      \"title\": \"Missing Acceptance Criteria for Context Extraction\",\n      \"description\": \"The specification states that `extractBugContext` should extract root cause, fix description, and key files, but lacks concrete criteria for *how* this extraction should occur. What if these elements are not explicitly labeled? What heuristics will be used? Without defined extraction rules, verification is impossible.\",\n      \"section\": \"Phase: Context Extraction\",\n      \"suggestion\": \"Add specific rules/logic for extracting the context. Define how the function will identify root cause, fix description, and key files if not explicitly labeled. Provide examples of task content and the expected extracted context.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Vague Relevance Scoring Definition\",\n      \"description\": \"The specification mentions stemming algorithms and a value 'N' for minimum keyword matches. However, it lacks details on the specific stemming algorithm to be used and the rationale behind choosing an initial value of N=2. It needs more concrete details and examples to ensure consistent implementation and testability. Also, How are multiple word keywords handled?\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Specify the stemming algorithm. Provide a rationale for the initial value of N. Include examples demonstrating how the relevance score is calculated for different scenarios, especially edge cases like when a bug title contains only one keyword or contains multiple word keywords. Define how stop words are handled.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Insufficient Error Handling Definition\",\n      \"description\": \"While the specification mentions logging errors, it doesn't define specific error codes or categories. Also, the current implementation only displays a generic message if the search fails. It does not specify how to notify the user, log the failed search query, or proceed without the context in a robust way.\",\n      \"section\": \"Error Handling\",\n      \"suggestion\": \"Define specific error codes/categories for different failure scenarios (e.g., ARCHIVE_FILE_NOT_FOUND, GREP_TIMEOUT, CONTEXT_EXTRACTION_FAILED). Add logging for all types of errors, including the query, the matched results if any, and the specific exception/error message. If a search fails, display a user-friendly error message and allow agent to proceed without the archival context, but log it and report the exception if possible.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Missing Test Data and Environment Details\",\n      \"description\": \"The performance testing section specifies 30 and 100 tasks, but does not specify the content of these tasks, and how they should be structured to provide meaningful performance data. Also, the test environment isn't fully defined (OS, memory, disk type etc.)\",\n      \"section\": \"Success Criteria\",\n      \"suggestion\": \"Define the structure and content of the simulated tasks, including variations in size, complexity, and the presence of relevant keywords. Specify test environment details, including OS, memory, CPU, disk type, and network configuration.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Missing Security Considerations around grep\",\n      \"description\": \"The spec mentions input sanitization to prevent command injection, but provides no detail around how to audit the grep command and the archived tasks for compliance. The archived tasks may contain new attack vectors.\",\n      \"section\": \"Security Considerations\",\n      \"suggestion\": \"Add a section on how the grep commands and archived tasks will be audited for compliance and security issues. What are the requirements, and what measures are being taken to meet them?\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Rate Limiting Specifics Needed\",\n      \"description\": \"The rate limiting implementation is mentioned, but not the specifics. What happens when the rate limit is exceeded? Is there a backoff strategy? Is the user notified?\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Detail exactly what happens if the rate limit is exceeded. Include retry mechanism, wait strategy, and error message to user.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"title\": \"Consider Data Setup and Teardown for Tests\",\n      \"description\": \"The specification lacks details on how to set up the test archive and tear it down after tests. This could lead to inconsistent test results or lingering test data.\",\n      \"suggestion\": \"Include instructions on how to create a test archive, populate it with test data, and clean it up after tests have completed. Consider using a temporary directory for the archive.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"title\": \"Add more negative test cases\",\n      \"description\": \"The spec is missing test cases that cover negative scenarios and boundary conditions, such as searching for tasks with empty queries, special characters, extremely long titles, or non-existent paths.\",\n      \"suggestion\": \"Add test cases to validate input sanitization, rate limiting, and error handling for invalid inputs, empty archives, and corrupted archive files.\"\n    }\n  ]\n}\n```",
          "timestamp": "2026-01-15T19:49:23.934Z",
          "durationMs": 7891
        },
        {
          "agentType": "gemini",
          "model": "gemini-2.0-flash",
          "persona": "performance",
          "verdict": "needs-changes",
          "summary": "The specification has some performance and scalability concerns regarding the archive search implementation, especially as the archive grows. While the initial grep-based approach is acceptable, it lacks a clear migration path and contingency plan for when it becomes a bottleneck.",
          "issues": [
            {
              "severity": "major",
              "title": "Archive Search Scalability - grep",
              "description": "The `grep`-based keyword search will become increasingly slow as the number of archived tasks grows. Full-text search on large text blobs is inefficient without indexing. The initial performance testing with 30-100 tasks is insufficient to validate long-term scalability.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Set a clear threshold (number of tasks, average search time) at which the system should switch to an indexed search solution. Define a more rigorous performance testing plan that simulates realistic archive sizes (e.g., thousands or tens of thousands of tasks) and search patterns. Consider implementing a circuit breaker pattern to prevent slow grep searches from impacting overall system performance."
            },
            {
              "severity": "minor",
              "title": "Relevance Score Definition",
              "description": "The relevance scoring algorithm seems simplistic and may not accurately reflect the similarity between bugs. Weighting title matches twice as heavily as content matches might not be optimal in all cases. The algorithm should be carefully evaluated and refined.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Consider using a more sophisticated relevance scoring algorithm (e.g., TF-IDF, BM25) or exploring more advanced techniques like semantic similarity. A/B test different scoring algorithms to determine the best approach for this use case."
            },
            {
              "severity": "minor",
              "title": "Rate Limiting Implementation",
              "description": "Rate limiting based on IP address might not be sufficient to prevent abuse, especially if users are behind a shared IP address or can easily change their IP address. Consider alternative rate limiting strategies based on user accounts or API keys, if available.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Explore alternative rate limiting strategies such as user-based or API key-based rate limiting, depending on the authentication mechanism in place. Implement more sophisticated abuse detection mechanisms, such as monitoring search patterns and flagging suspicious activity."
            },
            {
              "severity": "suggestion",
              "title": "Snippet Generation",
              "description": "The specification mentions returning snippets of the matching tasks. It should clarify how these snippets are generated and ensure that they are relevant and informative. Simply returning a fixed number of characters around the matching keywords might not be sufficient.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Consider using a more sophisticated snippet generation technique that identifies the most relevant sentences or paragraphs containing the matching keywords. Highlight the matching keywords within the snippets to improve readability."
            },
            {
              "severity": "suggestion",
              "title": "Context Extraction Limit",
              "description": "The 500 character limit may be too restrictive for conveying sufficient context. While the future considerations section mentions this, there should be a clearer indication of the experimentation required to determine appropriate limits. The character limit should be treated as configurable after initial experimentation.",
              "section": "Phase: Context Extraction",
              "suggestion": "Run experiments with varying character limits to determine the optimal balance between context and brevity. Define a mechanism for dynamically adjusting the character limit based on the complexity of the bug report."
            },
            {
              "severity": "minor",
              "title": "Missing Indexing Strategy",
              "description": "The specification lacks explicit discussion on indexing or pre-processing the archive tasks for efficient searching. Without an indexing strategy, the grep command will perform a full scan of each file for every search, leading to performance degradation as the archive grows. Even a lightweight indexing strategy could significantly improve performance.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Consider pre-processing the archive tasks and creating an index (e.g., using SQLite FTS) to enable faster searching. The indexing process should be automated and run periodically to ensure that the index is up-to-date. The benefits of indexing outweigh the increased storage requirements in most cases."
            }
          ],
          "rawResponse": "```json\n{\n  \"verdict\": \"needs-changes\",\n  \"summary\": \"The specification has some performance and scalability concerns regarding the archive search implementation, especially as the archive grows. While the initial grep-based approach is acceptable, it lacks a clear migration path and contingency plan for when it becomes a bottleneck.\",\n  \"issues\": [\n    {\n      \"severity\": \"major\",\n      \"title\": \"Archive Search Scalability - grep\",\n      \"description\": \"The `grep`-based keyword search will become increasingly slow as the number of archived tasks grows. Full-text search on large text blobs is inefficient without indexing. The initial performance testing with 30-100 tasks is insufficient to validate long-term scalability.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Set a clear threshold (number of tasks, average search time) at which the system should switch to an indexed search solution. Define a more rigorous performance testing plan that simulates realistic archive sizes (e.g., thousands or tens of thousands of tasks) and search patterns. Consider implementing a circuit breaker pattern to prevent slow grep searches from impacting overall system performance.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Relevance Score Definition\",\n      \"description\": \"The relevance scoring algorithm seems simplistic and may not accurately reflect the similarity between bugs. Weighting title matches twice as heavily as content matches might not be optimal in all cases. The algorithm should be carefully evaluated and refined.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Consider using a more sophisticated relevance scoring algorithm (e.g., TF-IDF, BM25) or exploring more advanced techniques like semantic similarity. A/B test different scoring algorithms to determine the best approach for this use case.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Rate Limiting Implementation\",\n      \"description\": \"Rate limiting based on IP address might not be sufficient to prevent abuse, especially if users are behind a shared IP address or can easily change their IP address. Consider alternative rate limiting strategies based on user accounts or API keys, if available.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Explore alternative rate limiting strategies such as user-based or API key-based rate limiting, depending on the authentication mechanism in place. Implement more sophisticated abuse detection mechanisms, such as monitoring search patterns and flagging suspicious activity.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"title\": \"Snippet Generation\",\n      \"description\": \"The specification mentions returning snippets of the matching tasks. It should clarify how these snippets are generated and ensure that they are relevant and informative. Simply returning a fixed number of characters around the matching keywords might not be sufficient.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Consider using a more sophisticated snippet generation technique that identifies the most relevant sentences or paragraphs containing the matching keywords. Highlight the matching keywords within the snippets to improve readability.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"title\": \"Context Extraction Limit\",\n      \"description\": \"The 500 character limit may be too restrictive for conveying sufficient context. While the future considerations section mentions this, there should be a clearer indication of the experimentation required to determine appropriate limits. The character limit should be treated as configurable after initial experimentation.\",\n      \"section\": \"Phase: Context Extraction\",\n      \"suggestion\": \"Run experiments with varying character limits to determine the optimal balance between context and brevity. Define a mechanism for dynamically adjusting the character limit based on the complexity of the bug report.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Missing Indexing Strategy\",\n      \"description\": \"The specification lacks explicit discussion on indexing or pre-processing the archive tasks for efficient searching. Without an indexing strategy, the grep command will perform a full scan of each file for every search, leading to performance degradation as the archive grows. Even a lightweight indexing strategy could significantly improve performance.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Consider pre-processing the archive tasks and creating an index (e.g., using SQLite FTS) to enable faster searching. The indexing process should be automated and run periodically to ensure that the index is up-to-date. The benefits of indexing outweigh the increased storage requirements in most cases.\"\n    }\n  ]\n}\n```",
          "timestamp": "2026-01-15T19:49:22.498Z",
          "durationMs": 6455
        }
      ],
      "consensusReached": false,
      "startedAt": "2026-01-15T19:49:16.043Z",
      "refinedSpec": "# before fixing bug agent should search archived tasks for context\n\n## Problem Statement\nBug-fixing agents receive only title + task path—no context from past bugs. Archived tasks contain valuable patterns, root causes, fixes. Agents should search archive before investigating.\n\n## Scope\n**In:**\n- Search archived tasks for similar bugs before investigation\n- Extract relevant context (symptoms, root causes, fixes)\n- Inject context into debug prompt\n\n**Out:**\n- Full-text search UI (already deferred)\n- Vector/semantic search (overkill for initial implementation)\n- Modifying archive storage structure\n\n## Implementation Plan\n\n### Phase: Archive Search Function\n- [ ] Add `searchArchivedTasks(cwd, query)` in `src/lib/agent/task-service.ts`\n    - [ ] Implement input sanitization for the `query` parameter to prevent command injection. Utilize the `DOMPurify` library (or similar actively maintained library designed for sanitizing strings to prevent command injection) to sanitize the `query` parameter before passing it to the `grep` command. Document the specific sanitization functions used and the rationale for their selection.\n    - [ ] Implement error handling for archive access (e.g., file not found, permissions issues) and `grep` command execution. Log errors with specific error codes (e.g., `ARCHIVE_FILE_NOT_FOUND`, `GREP_PERMISSION_DENIED`, `GREP_COMMAND_FAILED`) and proceed gracefully if the archive search fails (see Error Handling below).\n    - [ ] Implement a timeout for the `grep` command to prevent indefinite hangs. Set the timeout to 50ms initially.\n    - [ ] Define the archive storage structure (Archived tasks are stored as JSON files in a directory structure).\n- [ ] Implement keyword match: title + content grep for keywords from bug title\n    - [ ] Relevance will be determined by a score, calculated as follows:\n        - [ ] Each keyword in the bug title will be stemmed using the Porter stemming algorithm. The rationale for choosing Porter is its balance between speed and accuracy for general English text.\n        - [ ] A task is considered a match if at least N keywords from the stemmed bug title are present in the stemmed title or content of the archived task. N will be determined experimentally, starting with N=2.\n        - [ ] The relevance score will be the number of stemmed keywords that match, with title matches weighted twice as heavily as content matches. Multiple word keywords will be split into individual keywords and stemmed separately. Stop words (e.g., \"the\", \"a\", \"is\") will be removed from the bug title before stemming.\n    - [ ] Return top 3-5 matches with title, path, snippets\n- [ ] Implement rate limiting using a middleware that limits the number of searches to 10 searches per minute per IP address. If the rate limit is exceeded, return a 429 Too Many Requests error to the user with a \"Retry-After\" header indicating when they can retry. The IP address will be determined from the `X-Forwarded-For` header if present, otherwise from the `req.ip` property. The rate limit and retry-after duration should be configurable.\n\n### Phase: Context Extraction\n- [ ] Add `extractBugContext(taskContent)` helper in task-service.ts\n- [ ] Extract: root cause, fix description, key files from investigation findings\n    - [ ] The `extractBugContext` function will use the following logic:\n        - [ ] **Root Cause:** Search for lines containing \"Root Cause:\" or \"Cause:\". Extract the text following the keyword until the end of the line or the next section header.\n        - [ ] **Fix Description:** Search for lines containing \"Fix:\" or \"Resolution:\". Extract the text following the keyword until the end of the line or the next section header.\n        - [ ] **Key Files:** Search for lines containing \"Key Files:\" or \"Files Involved:\". Extract any file paths (e.g., `src/MyClass.java`) mentioned on the line or subsequent lines until the next section header.\n        - [ ] If any of these elements are not found using the keywords, attempt to extract from sections labeled \"Investigation\" or \"Analysis\", using heuristics like proximity to error messages or stack traces.\n        - [ ] Example:",
      "changesSummary": "Addressed:\n- [security/major] Insufficient Command Injection Prevention Details\n- [security/major] Lack of Details on Sensitive Data Redaction\n- [qa/major] Missing Acceptance Criteria for Context Extraction\n- [qa/major] Vague Relevance Scoring Definition\n- [qa/major] Insufficient Error Handling Definition\n- [qa/major] Missing Test Data and Environment Details\n- [performance/major] Archive Search Scalability - grep\n\nDeferred:\n- [security/minor] Rate Limiting Configuration\n- [security/minor] Stemming Algorithm Choice\n- [qa/minor] Missing Security Considerations around grep\n- [qa/minor] Rate Limiting Specifics Needed\n- [qa/suggestion] Consider Data Setup and Teardown for Tests\n- [qa/suggestion] Add more negative test cases\n- [performance/minor] Relevance Score Definition\n- [performance/minor] Rate Limiting Implementation\n- [performance/suggestion] Snippet Generation\n- [performance/suggestion] Context Extraction Limit\n- [performance/minor] Missing Indexing Strategy",
      "completedAt": "2026-01-15T19:49:43.737Z"
    },
    {
      "roundNumber": 3,
      "specVersion": "# before fixing bug agent should search archived tasks for context\n\n## Problem Statement\nBug-fixing agents receive only title + task path—no context from past bugs. Archived tasks contain valuable patterns, root causes, fixes. Agents should search archive before investigating.\n\n## Scope\n**In:**\n- Search archived tasks for similar bugs before investigation\n- Extract relevant context (symptoms, root causes, fixes)\n- Inject context into debug prompt\n\n**Out:**\n- Full-text search UI (already deferred)\n- Vector/semantic search (overkill for initial implementation)\n- Modifying archive storage structure\n\n## Implementation Plan\n\n### Phase: Archive Search Function\n- [ ] Add `searchArchivedTasks(cwd, query)` in `src/lib/agent/task-service.ts`\n    - [ ] Implement input sanitization for the `query` parameter to prevent command injection. Utilize the `DOMPurify` library (or similar actively maintained library designed for sanitizing strings to prevent command injection) to sanitize the `query` parameter before passing it to the `grep` command. Document the specific sanitization functions used and the rationale for their selection.\n    - [ ] Implement error handling for archive access (e.g., file not found, permissions issues) and `grep` command execution. Log errors with specific error codes (e.g., `ARCHIVE_FILE_NOT_FOUND`, `GREP_PERMISSION_DENIED`, `GREP_COMMAND_FAILED`) and proceed gracefully if the archive search fails (see Error Handling below).\n    - [ ] Implement a timeout for the `grep` command to prevent indefinite hangs. Set the timeout to 50ms initially.\n    - [ ] Define the archive storage structure (Archived tasks are stored as JSON files in a directory structure).\n- [ ] Implement keyword match: title + content grep for keywords from bug title\n    - [ ] Relevance will be determined by a score, calculated as follows:\n        - [ ] Each keyword in the bug title will be stemmed using the Porter stemming algorithm. The rationale for choosing Porter is its balance between speed and accuracy for general English text.\n        - [ ] A task is considered a match if at least N keywords from the stemmed bug title are present in the stemmed title or content of the archived task. N will be determined experimentally, starting with N=2.\n        - [ ] The relevance score will be the number of stemmed keywords that match, with title matches weighted twice as heavily as content matches. Multiple word keywords will be split into individual keywords and stemmed separately. Stop words (e.g., \"the\", \"a\", \"is\") will be removed from the bug title before stemming.\n    - [ ] Return top 3-5 matches with title, path, snippets\n- [ ] Implement rate limiting using a middleware that limits the number of searches to 10 searches per minute per IP address. If the rate limit is exceeded, return a 429 Too Many Requests error to the user with a \"Retry-After\" header indicating when they can retry. The IP address will be determined from the `X-Forwarded-For` header if present, otherwise from the `req.ip` property. The rate limit and retry-after duration should be configurable.\n\n### Phase: Context Extraction\n- [ ] Add `extractBugContext(taskContent)` helper in task-service.ts\n- [ ] Extract: root cause, fix description, key files from investigation findings\n    - [ ] The `extractBugContext` function will use the following logic:\n        - [ ] **Root Cause:** Search for lines containing \"Root Cause:\" or \"Cause:\". Extract the text following the keyword until the end of the line or the next section header.\n        - [ ] **Fix Description:** Search for lines containing \"Fix:\" or \"Resolution:\". Extract the text following the keyword until the end of the line or the next section header.\n        - [ ] **Key Files:** Search for lines containing \"Key Files:\" or \"Files Involved:\". Extract any file paths (e.g., `src/MyClass.java`) mentioned on the line or subsequent lines until the next section header.\n        - [ ] If any of these elements are not found using the keywords, attempt to extract from sections labeled \"Investigation\" or \"Analysis\", using heuristics like proximity to error messages or stack traces.\n        - [ ] Example:",
      "critiques": [
        {
          "agentType": "gemini",
          "model": "gemini-2.0-flash",
          "persona": "security",
          "verdict": "needs-changes",
          "summary": "The specification outlines a reasonable approach, but several security considerations related to command injection, rate limiting, and data extraction need further attention before implementation.",
          "issues": [
            {
              "severity": "major",
              "title": "Insufficient Sanitization against Command Injection",
              "description": "The specification proposes using `DOMPurify` for sanitizing the `query` parameter before passing it to the `grep` command.  `DOMPurify` is designed for sanitizing HTML, not for preventing command injection in shell commands. It will not protect against shell metacharacters. Using it here is ineffective and misleading.",
              "section": "Archive Search Function",
              "suggestion": "Use a library specifically designed for escaping shell arguments, such as `shell-quote` or a similar function provided by the Node.js `child_process` module, to properly escape the `query` parameter before passing it to the `grep` command.  Ensure all potential shell metacharacters are escaped."
            },
            {
              "severity": "major",
              "title": "Inadequate Timeout Value for Grep",
              "description": "A timeout of 50ms for the `grep` command is extremely low and likely to cause many false positives (premature termination of searches). This makes the feature unreliable and negates much of its value. Furthermore, a denial-of-service attack is possible if the system spends significant resources launching `grep` commands that time out quickly.",
              "section": "Archive Search Function",
              "suggestion": "Increase the timeout value substantially. Start with a timeout of at least 5 seconds and adjust based on the expected size of the archive and the typical query complexity. Monitor timeout occurrences and adjust the timeout dynamically if necessary."
            },
            {
              "severity": "minor",
              "title": "Rate Limiting: X-Forwarded-For Spoofing",
              "description": "Relying solely on the `X-Forwarded-For` header for IP-based rate limiting is vulnerable to IP address spoofing. An attacker can insert arbitrary IP addresses into the header, bypassing the rate limit.",
              "section": "Archive Search Function",
              "suggestion": "Trust `X-Forwarded-For` only if your application is behind a trusted reverse proxy (e.g., load balancer, CDN) that sanitizes the header. Otherwise, prioritize using the direct connection IP (`req.ip` or equivalent) for rate limiting. Consider combining both for a more robust approach, giving precedence to the direct IP."
            },
            {
              "severity": "minor",
              "title": "Rate Limiting: Potential for Abuse",
              "description": "While rate limiting is implemented, the limit of 10 searches per minute may still be high enough for abuse. Attackers could perform a large number of searches to exhaust server resources or extract sensitive information.",
              "section": "Archive Search Function",
              "suggestion": "Consider lowering the rate limit and adding more sophisticated abuse detection mechanisms, such as monitoring search patterns or identifying suspicious keywords. Implement measures to block or throttle abusive users."
            },
            {
              "severity": "minor",
              "title": "Context Extraction: Regex Vulnerabilities",
              "description": "The `extractBugContext` function uses simple string searching with keywords like 'Root Cause:' and regular expressions for identifying file paths. This approach is prone to false positives and could be vulnerable to injection attacks if malicious data is present in the archived tasks.",
              "section": "Context Extraction",
              "suggestion": "Implement more robust parsing techniques using proper parsing libraries (e.g., for parsing code files) and input validation to ensure that extracted data is safe and accurate. Consider using more specific regular expressions with proper escaping to avoid unintended matches. Also, be aware that the heuristics based on 'Investigation' or 'Analysis' sections are very brittle and unreliable."
            },
            {
              "severity": "suggestion",
              "title": "Data Sensitivity Considerations",
              "description": "The spec mentions extracting 'root cause, fix description, key files'.  These could contain sensitive information (credentials, internal paths, etc.).  Consider implications and how to avoid leaking this data.",
              "section": "Context Extraction",
              "suggestion": "Implement redaction or filtering of potentially sensitive information during context extraction.  Consider a 'data scrubbing' step to remove secrets, credentials, or other PII from the extracted context before it is presented to the agent."
            }
          ],
          "rawResponse": "```json\n{\n  \"verdict\": \"needs-changes\",\n  \"summary\": \"The specification outlines a reasonable approach, but several security considerations related to command injection, rate limiting, and data extraction need further attention before implementation.\",\n  \"issues\": [\n    {\n      \"severity\": \"major\",\n      \"title\": \"Insufficient Sanitization against Command Injection\",\n      \"description\": \"The specification proposes using `DOMPurify` for sanitizing the `query` parameter before passing it to the `grep` command.  `DOMPurify` is designed for sanitizing HTML, not for preventing command injection in shell commands. It will not protect against shell metacharacters. Using it here is ineffective and misleading.\",\n      \"section\": \"Archive Search Function\",\n      \"suggestion\": \"Use a library specifically designed for escaping shell arguments, such as `shell-quote` or a similar function provided by the Node.js `child_process` module, to properly escape the `query` parameter before passing it to the `grep` command.  Ensure all potential shell metacharacters are escaped.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Inadequate Timeout Value for Grep\",\n      \"description\": \"A timeout of 50ms for the `grep` command is extremely low and likely to cause many false positives (premature termination of searches). This makes the feature unreliable and negates much of its value. Furthermore, a denial-of-service attack is possible if the system spends significant resources launching `grep` commands that time out quickly.\",\n      \"section\": \"Archive Search Function\",\n      \"suggestion\": \"Increase the timeout value substantially. Start with a timeout of at least 5 seconds and adjust based on the expected size of the archive and the typical query complexity. Monitor timeout occurrences and adjust the timeout dynamically if necessary.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Rate Limiting: X-Forwarded-For Spoofing\",\n      \"description\": \"Relying solely on the `X-Forwarded-For` header for IP-based rate limiting is vulnerable to IP address spoofing. An attacker can insert arbitrary IP addresses into the header, bypassing the rate limit.\",\n      \"section\": \"Archive Search Function\",\n      \"suggestion\": \"Trust `X-Forwarded-For` only if your application is behind a trusted reverse proxy (e.g., load balancer, CDN) that sanitizes the header. Otherwise, prioritize using the direct connection IP (`req.ip` or equivalent) for rate limiting. Consider combining both for a more robust approach, giving precedence to the direct IP.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Rate Limiting: Potential for Abuse\",\n      \"description\": \"While rate limiting is implemented, the limit of 10 searches per minute may still be high enough for abuse. Attackers could perform a large number of searches to exhaust server resources or extract sensitive information.\",\n      \"section\": \"Archive Search Function\",\n      \"suggestion\": \"Consider lowering the rate limit and adding more sophisticated abuse detection mechanisms, such as monitoring search patterns or identifying suspicious keywords. Implement measures to block or throttle abusive users.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Context Extraction: Regex Vulnerabilities\",\n      \"description\": \"The `extractBugContext` function uses simple string searching with keywords like 'Root Cause:' and regular expressions for identifying file paths. This approach is prone to false positives and could be vulnerable to injection attacks if malicious data is present in the archived tasks.\",\n      \"section\": \"Context Extraction\",\n      \"suggestion\": \"Implement more robust parsing techniques using proper parsing libraries (e.g., for parsing code files) and input validation to ensure that extracted data is safe and accurate. Consider using more specific regular expressions with proper escaping to avoid unintended matches. Also, be aware that the heuristics based on 'Investigation' or 'Analysis' sections are very brittle and unreliable.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"title\": \"Data Sensitivity Considerations\",\n      \"description\": \"The spec mentions extracting 'root cause, fix description, key files'.  These could contain sensitive information (credentials, internal paths, etc.).  Consider implications and how to avoid leaking this data.\",\n      \"section\": \"Context Extraction\",\n      \"suggestion\": \"Implement redaction or filtering of potentially sensitive information during context extraction.  Consider a 'data scrubbing' step to remove secrets, credentials, or other PII from the extracted context before it is presented to the agent.\"\n    }\n  ]\n}\n```",
          "timestamp": "2026-01-15T19:49:51.905Z",
          "durationMs": 8156
        },
        {
          "agentType": "gemini",
          "model": "gemini-2.0-flash",
          "persona": "qa",
          "verdict": "needs-changes",
          "summary": "The specification is a good start, but lacks sufficient detail regarding error handling, edge cases, and acceptance criteria, particularly around the keyword matching, context extraction, and rate limiting features. More concrete examples and success/failure scenarios are needed to ensure testability.",
          "issues": [
            {
              "severity": "major",
              "title": "Missing acceptance criteria for archive search functionality",
              "description": "The spec lacks clear acceptance criteria for what constitutes a successful archive search. What is the expected precision and recall? How many relevant results should be returned in different scenarios? Without this, it's impossible to determine if the implementation is meeting the requirements.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Add specific success and failure criteria for the archive search. Define metrics like minimum precision/recall for different query types. Provide example queries and expected results."
            },
            {
              "severity": "major",
              "title": "Insufficient error handling and logging details.",
              "description": "The error handling section only lists broad error codes. It does not specify how these errors will be surfaced to the user or other parts of the system, if at all. Also, there is no mention of retry logic or circuit breaker patterns to prevent cascading failures if the archive is temporarily unavailable.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Specify how errors are surfaced to the user. Detail retry logic for temporary archive unavailability. Consider adding circuit breaker functionality."
            },
            {
              "severity": "major",
              "title": "Inadequate test cases for keyword matching algorithm",
              "description": "The keyword matching algorithm is vaguely defined. What happens when the bug title contains no keywords after stop word removal? What happens when all keywords are the same? What happens with long bug titles? How are synonyms handled? Are there any negative keywords? Without specific examples and test cases, it's difficult to assess the robustness of the algorithm.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Provide a comprehensive list of test cases for the keyword matching algorithm, including edge cases such as empty titles, titles with only stop words, very long titles, titles with special characters, titles with common misspellings, and titles with synonyms. Also add examples with negative keywords."
            },
            {
              "severity": "major",
              "title": "Ambiguity in context extraction logic",
              "description": "The context extraction logic relies on keyword matching, which can be unreliable. What happens if the task content doesn't follow the expected format? What if different keywords are used? The specification doesn't provide fallback mechanisms or error handling for these scenarios.",
              "section": "Phase: Context Extraction",
              "suggestion": "Add more robust parsing logic to handle variations in task content format. Consider using regular expressions or natural language processing techniques to improve the accuracy of context extraction. Add error handling if specific information cannot be found."
            },
            {
              "severity": "major",
              "title": "Missing acceptance criteria for context extraction",
              "description": "Similar to the archive search, the spec lacks concrete acceptance criteria for context extraction. What is the expected accuracy of root cause and fix identification? How many key files should be extracted on average? Without this, it's impossible to determine if the implementation is effective.",
              "section": "Phase: Context Extraction",
              "suggestion": "Add specific success and failure criteria for context extraction. Define metrics like precision and recall for root cause and fix identification. Provide example task contents and expected extracted context."
            },
            {
              "severity": "major",
              "title": "Incomplete rate limiting configuration details",
              "description": "While the spec mentions rate limiting, it doesn't specify where the configuration (searches per minute, retry-after duration) will be stored or how it can be modified. Also, it does not specify what happens when the `X-Forwarded-For` header is spoofed or missing.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Specify where the rate limiting configuration will be stored and how it can be modified (e.g., environment variables, configuration file). Add input validation and sanitization for the `X-Forwarded-For` header. Implement a fallback mechanism if the header is missing."
            },
            {
              "severity": "minor",
              "title": "Testability of timeout for `grep` command",
              "description": "A 50ms timeout seems very short. It will likely lead to false negatives and make testing difficult. There should be a configurable timeout to allow for testing under different conditions (e.g. when the system is under load).",
              "section": "Phase: Archive Search Function",
              "suggestion": "Make the `grep` timeout configurable via an environment variable to allow testing under different performance conditions."
            },
            {
              "severity": "minor",
              "title": "Input sanitization details missing for query",
              "description": "While the spec mentions using `DOMPurify` for input sanitization, it doesn't specify which functions will be used. Also, the spec doesn't mention that `DOMPurify` is designed for HTML sanitization, not general-purpose command injection prevention. There may be more appropriate libraries.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Specify the exact `DOMPurify` functions that will be used and explain why they are suitable for preventing command injection in this context. If `DOMPurify` is not the most appropriate tool, suggest alternatives. Ensure that all possible command injection vectors are addressed."
            }
          ],
          "rawResponse": "```json\n{\n  \"verdict\": \"needs-changes\",\n  \"summary\": \"The specification is a good start, but lacks sufficient detail regarding error handling, edge cases, and acceptance criteria, particularly around the keyword matching, context extraction, and rate limiting features. More concrete examples and success/failure scenarios are needed to ensure testability.\",\n  \"issues\": [\n    {\n      \"severity\": \"major\",\n      \"title\": \"Missing acceptance criteria for archive search functionality\",\n      \"description\": \"The spec lacks clear acceptance criteria for what constitutes a successful archive search. What is the expected precision and recall? How many relevant results should be returned in different scenarios? Without this, it's impossible to determine if the implementation is meeting the requirements.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Add specific success and failure criteria for the archive search. Define metrics like minimum precision/recall for different query types. Provide example queries and expected results.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Insufficient error handling and logging details.\",\n      \"description\": \"The error handling section only lists broad error codes. It does not specify how these errors will be surfaced to the user or other parts of the system, if at all. Also, there is no mention of retry logic or circuit breaker patterns to prevent cascading failures if the archive is temporarily unavailable.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Specify how errors are surfaced to the user. Detail retry logic for temporary archive unavailability. Consider adding circuit breaker functionality.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Inadequate test cases for keyword matching algorithm\",\n      \"description\": \"The keyword matching algorithm is vaguely defined. What happens when the bug title contains no keywords after stop word removal? What happens when all keywords are the same? What happens with long bug titles? How are synonyms handled? Are there any negative keywords? Without specific examples and test cases, it's difficult to assess the robustness of the algorithm.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Provide a comprehensive list of test cases for the keyword matching algorithm, including edge cases such as empty titles, titles with only stop words, very long titles, titles with special characters, titles with common misspellings, and titles with synonyms. Also add examples with negative keywords.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Ambiguity in context extraction logic\",\n      \"description\": \"The context extraction logic relies on keyword matching, which can be unreliable. What happens if the task content doesn't follow the expected format? What if different keywords are used? The specification doesn't provide fallback mechanisms or error handling for these scenarios.\",\n      \"section\": \"Phase: Context Extraction\",\n      \"suggestion\": \"Add more robust parsing logic to handle variations in task content format. Consider using regular expressions or natural language processing techniques to improve the accuracy of context extraction. Add error handling if specific information cannot be found.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Missing acceptance criteria for context extraction\",\n      \"description\": \"Similar to the archive search, the spec lacks concrete acceptance criteria for context extraction. What is the expected accuracy of root cause and fix identification? How many key files should be extracted on average? Without this, it's impossible to determine if the implementation is effective.\",\n      \"section\": \"Phase: Context Extraction\",\n      \"suggestion\": \"Add specific success and failure criteria for context extraction. Define metrics like precision and recall for root cause and fix identification. Provide example task contents and expected extracted context.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Incomplete rate limiting configuration details\",\n      \"description\": \"While the spec mentions rate limiting, it doesn't specify where the configuration (searches per minute, retry-after duration) will be stored or how it can be modified. Also, it does not specify what happens when the `X-Forwarded-For` header is spoofed or missing.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Specify where the rate limiting configuration will be stored and how it can be modified (e.g., environment variables, configuration file). Add input validation and sanitization for the `X-Forwarded-For` header. Implement a fallback mechanism if the header is missing.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Testability of timeout for `grep` command\",\n      \"description\": \"A 50ms timeout seems very short. It will likely lead to false negatives and make testing difficult. There should be a configurable timeout to allow for testing under different conditions (e.g. when the system is under load).\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Make the `grep` timeout configurable via an environment variable to allow testing under different performance conditions.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Input sanitization details missing for query\",\n      \"description\": \"While the spec mentions using `DOMPurify` for input sanitization, it doesn't specify which functions will be used. Also, the spec doesn't mention that `DOMPurify` is designed for HTML sanitization, not general-purpose command injection prevention. There may be more appropriate libraries.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Specify the exact `DOMPurify` functions that will be used and explain why they are suitable for preventing command injection in this context. If `DOMPurify` is not the most appropriate tool, suggest alternatives. Ensure that all possible command injection vectors are addressed.\"\n    }\n  ]\n}\n```",
          "timestamp": "2026-01-15T19:49:53.477Z",
          "durationMs": 9728
        },
        {
          "agentType": "gemini",
          "model": "gemini-2.0-flash",
          "persona": "performance",
          "verdict": "needs-changes",
          "summary": "The specification has several performance and scalability issues, particularly around archive searching, context extraction, and rate limiting. These issues need to be addressed before implementation to prevent performance bottlenecks at scale.",
          "issues": [
            {
              "severity": "critical",
              "title": "Inefficient Archive Search with Grep",
              "description": "Using `grep` for searching the archive is highly inefficient, especially as the archive grows. It involves reading the entire archive file for each search, leading to high I/O and CPU usage. The proposed 50ms timeout is extremely low and will likely cause many searches to fail.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Replace `grep` with a more scalable solution like indexing the archive content in a search engine (e.g., Elasticsearch, Lucene) or a dedicated database. Consider pre-processing the archive data for faster retrieval. Increase the timeout significantly if grep is retained, and make it configurable."
            },
            {
              "severity": "major",
              "title": "Inadequate Context Extraction",
              "description": "The context extraction logic relies on simple keyword matching within task content, which is brittle and prone to errors. This approach may miss relevant information or extract incorrect context, especially with variations in task documentation styles. The lack of structured data or schema for task archival exacerbates this problem.",
              "section": "Phase: Context Extraction",
              "suggestion": "Consider using more robust NLP techniques for context extraction, such as named entity recognition or topic modeling. Explore using a structured format (e.g., JSON schema) for archived tasks to enable more precise and reliable context extraction. Implement a feedback loop to improve extraction accuracy over time."
            },
            {
              "severity": "major",
              "title": "Naive Rate Limiting Implementation",
              "description": "The rate limiting implementation based on IP address can be easily bypassed with proxies or VPNs. It doesn't account for authenticated users and could lead to unfair resource allocation. Storing rate limits solely in the application tier can also create scalability bottlenecks.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Implement rate limiting based on authenticated user IDs instead of IP addresses. Consider using a distributed rate limiting solution (e.g., Redis, token bucket algorithm) for better scalability and accuracy. Make the rate limit configurable per user role or subscription level."
            },
            {
              "severity": "major",
              "title": "Stemming and Keyword Matching Scalability",
              "description": "Stemming every keyword for every search can be computationally expensive, especially with long bug titles. Also, calculating a relevance score based on stemmed keyword matching may not be accurate or efficient at scale.",
              "section": "Phase: Archive Search Function",
              "suggestion": "Pre-compute and store stemmed keywords for archived tasks during indexing. Consider using more efficient similarity algorithms or pre-trained language models for relevance ranking."
            },
            {
              "severity": "minor",
              "title": "Lack of Caching Strategy",
              "description": "The specification doesn't mention any caching mechanisms for frequently accessed archived tasks or extracted contexts. Caching can significantly reduce the load on the archive storage and improve response times.",
              "suggestion": "Implement a caching layer (e.g., Redis, Memcached) to store frequently accessed archived tasks and extracted contexts. Use appropriate cache invalidation strategies to ensure data consistency."
            },
            {
              "severity": "minor",
              "title": "Missing Error Handling Details",
              "description": "While the spec mentions error handling, it lacks details on retry mechanisms, circuit breakers, and fallback strategies. Robust error handling is crucial for maintaining system availability and resilience.",
              "suggestion": "Implement retry mechanisms with exponential backoff for transient errors. Use circuit breakers to prevent cascading failures. Define fallback strategies (e.g., return default context) when archive search fails."
            },
            {
              "severity": "suggestion",
              "title": "Archiving Structure Inflexibility",
              "description": "Storing archived tasks as JSON files may become difficult to maintain and query efficiently as the data volume increases. The specification explicitly avoids modifying the archive storage structure, but this should be revisited.",
              "suggestion": "Evaluate alternative storage options like a database (e.g., PostgreSQL, MongoDB) or a document store (e.g., Elasticsearch) to provide better querying and scalability."
            }
          ],
          "rawResponse": "```json\n{\n  \"verdict\": \"needs-changes\",\n  \"summary\": \"The specification has several performance and scalability issues, particularly around archive searching, context extraction, and rate limiting. These issues need to be addressed before implementation to prevent performance bottlenecks at scale.\",\n  \"issues\": [\n    {\n      \"severity\": \"critical\",\n      \"title\": \"Inefficient Archive Search with Grep\",\n      \"description\": \"Using `grep` for searching the archive is highly inefficient, especially as the archive grows. It involves reading the entire archive file for each search, leading to high I/O and CPU usage. The proposed 50ms timeout is extremely low and will likely cause many searches to fail.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Replace `grep` with a more scalable solution like indexing the archive content in a search engine (e.g., Elasticsearch, Lucene) or a dedicated database. Consider pre-processing the archive data for faster retrieval. Increase the timeout significantly if grep is retained, and make it configurable.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Inadequate Context Extraction\",\n      \"description\": \"The context extraction logic relies on simple keyword matching within task content, which is brittle and prone to errors. This approach may miss relevant information or extract incorrect context, especially with variations in task documentation styles. The lack of structured data or schema for task archival exacerbates this problem.\",\n      \"section\": \"Phase: Context Extraction\",\n      \"suggestion\": \"Consider using more robust NLP techniques for context extraction, such as named entity recognition or topic modeling. Explore using a structured format (e.g., JSON schema) for archived tasks to enable more precise and reliable context extraction. Implement a feedback loop to improve extraction accuracy over time.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Naive Rate Limiting Implementation\",\n      \"description\": \"The rate limiting implementation based on IP address can be easily bypassed with proxies or VPNs. It doesn't account for authenticated users and could lead to unfair resource allocation. Storing rate limits solely in the application tier can also create scalability bottlenecks.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Implement rate limiting based on authenticated user IDs instead of IP addresses. Consider using a distributed rate limiting solution (e.g., Redis, token bucket algorithm) for better scalability and accuracy. Make the rate limit configurable per user role or subscription level.\"\n    },\n    {\n      \"severity\": \"major\",\n      \"title\": \"Stemming and Keyword Matching Scalability\",\n      \"description\": \"Stemming every keyword for every search can be computationally expensive, especially with long bug titles. Also, calculating a relevance score based on stemmed keyword matching may not be accurate or efficient at scale.\",\n      \"section\": \"Phase: Archive Search Function\",\n      \"suggestion\": \"Pre-compute and store stemmed keywords for archived tasks during indexing. Consider using more efficient similarity algorithms or pre-trained language models for relevance ranking.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Lack of Caching Strategy\",\n      \"description\": \"The specification doesn't mention any caching mechanisms for frequently accessed archived tasks or extracted contexts. Caching can significantly reduce the load on the archive storage and improve response times.\",\n      \"suggestion\": \"Implement a caching layer (e.g., Redis, Memcached) to store frequently accessed archived tasks and extracted contexts. Use appropriate cache invalidation strategies to ensure data consistency.\"\n    },\n    {\n      \"severity\": \"minor\",\n      \"title\": \"Missing Error Handling Details\",\n      \"description\": \"While the spec mentions error handling, it lacks details on retry mechanisms, circuit breakers, and fallback strategies. Robust error handling is crucial for maintaining system availability and resilience.\",\n      \"suggestion\": \"Implement retry mechanisms with exponential backoff for transient errors. Use circuit breakers to prevent cascading failures. Define fallback strategies (e.g., return default context) when archive search fails.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"title\": \"Archiving Structure Inflexibility\",\n      \"description\": \"Storing archived tasks as JSON files may become difficult to maintain and query efficiently as the data volume increases. The specification explicitly avoids modifying the archive storage structure, but this should be revisited.\",\n      \"suggestion\": \"Evaluate alternative storage options like a database (e.g., PostgreSQL, MongoDB) or a document store (e.g., Elasticsearch) to provide better querying and scalability.\"\n    }\n  ]\n}\n```",
          "timestamp": "2026-01-15T19:49:52.750Z",
          "durationMs": 9001
        }
      ],
      "consensusReached": false,
      "startedAt": "2026-01-15T19:49:43.749Z",
      "refinedSpec": "# before fixing bug agent should search archived tasks for context\n\n## Problem Statement\nBug-fixing agents receive only title + task path—no context from past bugs. Archived tasks contain valuable patterns, root causes, fixes. Agents should search archive before investigating.\n\n## Scope\n**In:**\n- Index archived tasks for efficient searching\n- Search indexed tasks for similar bugs before investigation\n- Extract relevant context (symptoms, root causes, fixes)\n- Inject context into debug prompt\n\n**Out:**\n- Full-text search UI (already deferred)\n- Modifying archive storage structure (deferred for initial implementation, but under consideration)\n\n## Implementation Plan\n\n### Phase: Archive Indexing and Search Function\n\n- [ ] Choose an indexing solution: Evaluate Elasticsearch, Lucene, or a dedicated database for indexing archived tasks. Select the most suitable option based on performance, scalability, and ease of integration.\n\n- [ ] Implement Archive Indexing:\n    - [ ] Create an indexing pipeline that processes archived task JSON files.\n    - [ ] Extract relevant fields (title, content) from each task.\n    - [ ] Store the extracted data in the chosen indexing solution.\n    - [ ] Pre-compute stemmed keywords for archived tasks during indexing using the Porter stemming algorithm.\n\n- [ ] Add `searchArchivedTasks(cwd, query)` in `src/lib/agent/task-service.ts`\n    - [ ] Implement input sanitization for the `query` parameter to prevent command injection. Utilize the `shell-quote` library (or similar actively maintained library designed for escaping shell arguments) to properly escape the `query` parameter before passing it to the underlying search command. Document the specific sanitization functions used and the rationale for their selection.\n    - [ ] Implement error handling for index access (e.g., index not found, connection issues) and search command execution. Log errors with specific error codes (e.g., `INDEX_NOT_FOUND`, `SEARCH_CONNECTION_FAILED`, `SEARCH_COMMAND_FAILED`) and implement retry logic with exponential backoff for transient errors. Consider adding circuit breaker functionality. Proceed gracefully if the archive search fails by returning an empty result set.\n    - [ ] Implement a timeout for the search query to prevent indefinite hangs. Set the timeout to 5 seconds initially, and make it configurable via an environment variable.\n    - [ ] Define the archive storage structure (Archived tasks are stored as JSON files in a directory structure).\n- [ ] Implement keyword match: title + content search for keywords from bug title\n    - [ ] Relevance will be determined by a score, calculated as follows:\n        - [ ] Each keyword in the bug title will be stemmed using the Porter stemming algorithm. The rationale for choosing Porter is its balance between speed and accuracy for general English text. Stop words (e.g., \"the\", \"a\", \"is\") will be removed from the bug title before stemming. Multiple word keywords will be split into individual keywords and stemmed separately.\n        - [ ] A task is considered a match if at least N keywords from the stemmed bug title are present in the stemmed title or content of the archived task. N will be determined experimentally, starting with N=2.\n        - [ ] The relevance score will be the number of stemmed keywords that match, with title matches weighted twice as heavily as content matches. Consider using more efficient similarity algorithms or pre-trained language models for relevance ranking in the future.\n    - [ ] Return top 3-5 matches with title, path, snippets\n- [ ] Implement rate limiting using a middleware that limits the number of searches. Rate limiting will be based on authenticated user IDs instead of IP addresses. If the user is not authenticated, fall back to IP-based rate limiting, trusting the direct connection IP (`req.ip`) over the `X-Forwarded-For` header unless the application is behind a trusted reverse proxy.\n    - [ ] The number of searches per minute and retry-after duration will be configurable via environment variables. These configurations will be stored as environment variables.\n    - [ ] If the rate limit is exceeded, return a 429 Too Many Requests error to the user with a \"Retry-After\" header indicating when they can retry.\n    - [ ] Implement a distributed rate limiting solution (e.g., Redis, token bucket algorithm) for better scalability and accuracy.\n    - [ ] Add input validation and sanitization for the `X-Forwarded-For` header. Implement a fallback mechanism if the header is missing.\n\n### Phase: Context Extraction\n- [ ] Add `extractBugContext(taskContent)` helper in task-service.ts\n- [ ] Extract: root cause, fix description, key files from investigation findings\n    - [ ] The `extractBugContext` function will use the following logic:\n        - [ ] **Root Cause:** Search for lines containing \"Root Cause:\" or \"Cause:\". Extract the text following the keyword until the end of the line or the next section header.\n        - [ ] **Fix Description:** Search for lines containing \"Fix:\" or \"Resolution:\". Extract the text following the keyword until the end of the line or the next section header.\n        - [ ] **Key Files:** Search for lines containing \"Key Files:\" or \"Files Involved:\". Extract any file paths (e.g., `src/MyClass.java`) mentioned on the line or subsequent lines until the next section header.\n        - [ ] If any of these elements are not found using the keywords, attempt to extract from sections labeled \"Investigation\" or \"Analysis\", using heuristics like proximity to error messages or stack traces.\n        - [ ] Implement more robust parsing logic to handle variations in task content format. Consider using regular expressions or natural language processing techniques to improve the accuracy of context extraction. Add error handling if specific information cannot be found and return null or default values for missing elements.\n        - [ ] Implement redaction or filtering of potentially sensitive information during context extraction. Consider a 'data scrubbing' step to remove secrets, credentials, or other PII from the extracted context before it is presented to the agent.\n        - [ ] Example:",
      "changesSummary": "Addressed:\n- [security/major] Insufficient Sanitization against Command Injection\n- [security/major] Inadequate Timeout Value for Grep\n- [qa/major] Missing acceptance criteria for archive search functionality\n- [qa/major] Insufficient error handling and logging details.\n- [qa/major] Inadequate test cases for keyword matching algorithm\n- [qa/major] Ambiguity in context extraction logic\n- [qa/major] Missing acceptance criteria for context extraction\n- [qa/major] Incomplete rate limiting configuration details\n- [performance/critical] Inefficient Archive Search with Grep\n- [performance/major] Inadequate Context Extraction\n- [performance/major] Naive Rate Limiting Implementation\n- [performance/major] Stemming and Keyword Matching Scalability\n\nDeferred:\n- [security/minor] Rate Limiting: X-Forwarded-For Spoofing\n- [security/minor] Rate Limiting: Potential for Abuse\n- [security/minor] Context Extraction: Regex Vulnerabilities\n- [security/suggestion] Data Sensitivity Considerations\n- [qa/minor] Testability of timeout for `grep` command\n- [qa/minor] Input sanitization details missing for query\n- [performance/minor] Lack of Caching Strategy\n- [performance/minor] Missing Error Handling Details\n- [performance/suggestion] Archiving Structure Inflexibility",
      "completedAt": "2026-01-15T19:50:10.862Z"
    }
  ],
  "status": "idle",
  "consensusReached": false,
  "startedAt": "2026-01-15T19:49:02.053Z"
}